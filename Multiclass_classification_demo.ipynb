{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:21.961070Z",
     "start_time": "2019-07-13T16:54:19.605441Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import logging\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset/Downloaded from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:23.589331Z",
     "start_time": "2019-07-13T16:54:21.963135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['authors', 'category', 'date', 'headline', 'link', 'short_description'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news = pd.read_json(\"/Users/orah82/Downloads/News_Category_Dataset_v2.json\", lines = True)\n",
    "df_news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:24.402731Z",
     "start_time": "2019-07-13T16:54:24.315084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200853 entries, 0 to 200852\n",
      "Data columns (total 6 columns):\n",
      "authors              200853 non-null object\n",
      "category             200853 non-null object\n",
      "date                 200853 non-null datetime64[ns]\n",
      "headline             200853 non-null object\n",
      "link                 200853 non-null object\n",
      "short_description    200853 non-null object\n",
      "dtypes: datetime64[ns](1), object(5)\n",
      "memory usage: 9.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_news.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Rows of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:26.887388Z",
     "start_time": "2019-07-13T16:54:26.874673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Morgan Freeman 'Devastated' That Sexual Harass...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/morgan-fr...</td>\n",
       "      <td>\"It is not right to equate horrific incidents ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Donald Trump Is Lovin' New McDonald's Jingle I...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donald-tr...</td>\n",
       "      <td>It's catchy, all right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Todd Van Luling</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>What To Watch On Amazon Prime That’s New This ...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/amazon-pr...</td>\n",
       "      <td>There's a great mini-series joining this week.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Mike Myers Reveals He'd 'Like To' Do A Fourth ...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mike-myer...</td>\n",
       "      <td>Myer's kids may be pushing for a new \"Powers\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Todd Van Luling</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>What To Watch On Hulu That’s New This Week</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hulu-what...</td>\n",
       "      <td>You're getting a recent Academy Award-winning ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "5       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "6       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "7  Todd Van Luling  ENTERTAINMENT 2018-05-26   \n",
       "8    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "9  Todd Van Luling  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "5  Morgan Freeman 'Devastated' That Sexual Harass...   \n",
       "6  Donald Trump Is Lovin' New McDonald's Jingle I...   \n",
       "7  What To Watch On Amazon Prime That’s New This ...   \n",
       "8  Mike Myers Reveals He'd 'Like To' Do A Fourth ...   \n",
       "9         What To Watch On Hulu That’s New This Week   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "5  https://www.huffingtonpost.com/entry/morgan-fr...   \n",
       "6  https://www.huffingtonpost.com/entry/donald-tr...   \n",
       "7  https://www.huffingtonpost.com/entry/amazon-pr...   \n",
       "8  https://www.huffingtonpost.com/entry/mike-myer...   \n",
       "9  https://www.huffingtonpost.com/entry/hulu-what...   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1                           Of course it has a song.  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  \n",
       "5  \"It is not right to equate horrific incidents ...  \n",
       "6                            It's catchy, all right.  \n",
       "7     There's a great mini-series joining this week.  \n",
       "8  Myer's kids may be pushing for a new \"Powers\" ...  \n",
       "9  You're getting a recent Academy Award-winning ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:29.061260Z",
     "start_time": "2019-07-13T16:54:29.027689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIME' 'ENTERTAINMENT' 'WORLD NEWS' 'IMPACT' 'POLITICS' 'WEIRD NEWS'\n",
      " 'BLACK VOICES' 'WOMEN' 'COMEDY' 'QUEER VOICES' 'SPORTS' 'BUSINESS'\n",
      " 'TRAVEL' 'MEDIA' 'TECH' 'RELIGION' 'SCIENCE' 'LATINO VOICES' 'EDUCATION'\n",
      " 'COLLEGE' 'PARENTS' 'ARTS & CULTURE' 'STYLE' 'GREEN' 'TASTE'\n",
      " 'HEALTHY LIVING' 'THE WORLDPOST' 'GOOD NEWS' 'WORLDPOST' 'FIFTY' 'ARTS'\n",
      " 'WELLNESS' 'PARENTING' 'HOME & LIVING' 'STYLE & BEAUTY' 'DIVORCE'\n",
      " 'WEDDINGS' 'FOOD & DRINK' 'MONEY' 'ENVIRONMENT' 'CULTURE & ARTS']\n"
     ]
    }
   ],
   "source": [
    "print(df_news['category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:30.001076Z",
     "start_time": "2019-07-13T16:54:29.972432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_news['category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:31.347667Z",
     "start_time": "2019-07-13T16:54:30.887816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10b95bf60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news['category'].value_counts().plot(kind='bar', figsize=(18,12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge WorldPost and The WorldPost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:33.542329Z",
     "start_time": "2019-07-13T16:54:33.462252Z"
    }
   },
   "outputs": [],
   "source": [
    "#Bascially the same categories \n",
    "df_news.category = df_news.category.map(lambda x: \"WORLDPOST\" if x == \"THEWORLDPOST\" else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Headline and Short Description into new column named \"Text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:35.563325Z",
     "start_time": "2019-07-13T16:54:35.468951Z"
    }
   },
   "outputs": [],
   "source": [
    "df_news['text'] = df_news['headline'] +\" \"+ df_news['short_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load in NLTK utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:37.911028Z",
     "start_time": "2019-07-13T16:54:37.490365Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/orah82/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/orah82/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/orah82/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/orah82/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text cleaning function, removes stop words and non alphanumeric for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:40.446052Z",
     "start_time": "2019-07-13T16:54:40.440959Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:45.599735Z",
     "start_time": "2019-07-13T16:54:45.596052Z"
    }
   },
   "outputs": [],
   "source": [
    "my_sw = ['make', 'amp', 'news', 'new', 'time', 'u', 's', 'photos', 'get', 'say',]\n",
    "def black_txt(token):\n",
    "    return token not in stop_words and token not in list(string.punctuation) and len(token)>2 and token not in my_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:47.282520Z",
     "start_time": "2019-07-13T16:54:47.277746Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    clean_text = []\n",
    "    clean_text2 = []\n",
    "    text = re.sub(\"'\", \"\", text)\n",
    "    text=re.sub(\"\\\\d|\\\\W+\",\" \",text)\n",
    "    clean_text =[wn.lemmatize(word, pos=\"v\") for word in word_tokenize(text.lower()) if black_txt(word)]\n",
    "    clean_text2 = [word for word in clean_text if black_txt(word)]\n",
    "    return \" \".join(clean_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:48.641840Z",
     "start_time": "2019-07-13T16:54:48.631003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She left her husband. He killed their children. Just another day in America.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.short_description[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the data and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:52.535896Z",
     "start_time": "2019-07-13T16:54:52.532215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song Of course it has a song.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:54.762543Z",
     "start_time": "2019-07-13T16:54:53.370539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smith join diplo nicky jam world cup official song course song'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(df_news.text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:54:54.768447Z",
     "start_time": "2019-07-13T16:54:54.764128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mike myers reveal hed like fourth austin power film myers kid may push power film anyone'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(df_news.text[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we are going to create some news variables columns(like metadata) to improve the quality of our classifier with the help if textblob package. We will make:\n",
    "\n",
    "- Polarity: to check the sentiment of the text\n",
    "- Subjectivity: to check if the text is objective or subjective\n",
    "- Len: The number of word in the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:55:08.941680Z",
     "start_time": "2019-07-13T16:55:08.447384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugh Grant Carries For The First Time It Age 57 The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob((df_news.text[2]))\n",
    "str(blob.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:55:16.364625Z",
     "start_time": "2019-07-13T16:55:16.362110Z"
    }
   },
   "outputs": [],
   "source": [
    "def polarity_txt(text):\n",
    "    return TextBlob(text).sentiment[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:55:16.900960Z",
     "start_time": "2019-07-13T16:55:16.898673Z"
    }
   },
   "outputs": [],
   "source": [
    "def subj_txt(text):\n",
    "    return TextBlob(text).sentiment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:55:19.855231Z",
     "start_time": "2019-07-13T16:55:19.851674Z"
    }
   },
   "outputs": [],
   "source": [
    "def len_text(text):\n",
    "    if len(text.split())>0:\n",
    "        return len(set(clean_text(text).split()))/ len(text.split())\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:56:16.538192Z",
     "start_time": "2019-07-13T16:55:20.784884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "\n",
       "                                                text  polarity  \n",
       "0  There Were 2 Mass Shootings In Texas Last Week...     -0.05  \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...      0.00  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news['polarity'] = df_news['text'].apply(polarity_txt)\n",
    "df_news.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:57:07.739789Z",
     "start_time": "2019-07-13T16:56:16.540007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "\n",
       "                                                text  polarity  subjectivity  \n",
       "0  There Were 2 Mass Shootings In Texas Last Week...     -0.05      0.266667  \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...      0.00      0.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news['subjectivity'] = df_news['text'].apply(subj_txt)\n",
    "df_news.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:58:12.973244Z",
     "start_time": "2019-07-13T16:57:16.217572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>-0.051768</td>\n",
       "      <td>0.498737</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3  The actor gives Dems an ass-kicking for not fi...   \n",
       "4  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                                text  polarity  subjectivity  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week... -0.050000      0.266667   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...  0.000000      0.000000   \n",
       "2  Hugh Grant Marries For The First Time At Age 5...  0.250000      0.333333   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D... -0.051768      0.498737   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...  0.200000      0.200000   \n",
       "\n",
       "        len  \n",
       "0  0.444444  \n",
       "1  0.500000  \n",
       "2  0.560000  \n",
       "3  0.720000  \n",
       "4  0.576923  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news['len'] = df_news['text'].apply(len_text)\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the custom class for feature union transformer of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:59:30.216399Z",
     "start_time": "2019-07-13T16:59:30.210156Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "    \n",
    "\n",
    "# Extracts features from each document for DictVectorizer\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        return [{'pos': row['polarity'], 'sub': row['subjectivity'], 'len':row['len']} for _,row in data.iterrows()]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:02:57.200566Z",
     "start_time": "2019-07-13T17:02:57.194383Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "        #Pipeline to pull features from text\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer(min_df =3, max_df=0.2, max_features=None,\n",
    "                  strip_accents='unicode', analyzer = 'word', token_pattern=r'\\w{1,}',\n",
    "                  ngram_range=(1,10), use_idf=1, smooth_idf=1,sublinear_tf=1,\n",
    "                  stop_words= None, preprocessor=clean_text)),\n",
    "            ])),\n",
    "        #Pipeline to pull metadata features\n",
    "            ('stats', Pipeline([\n",
    "                ('selector', ItemSelector(key=['polarity', 'subjectivity', 'len'])),\n",
    "                ('stats', TextStats()), #returns a list of dicts\n",
    "                ('vect', DictVectorizer()), #puts dicts in feature matrix\n",
    "            ])),\n",
    "    \n",
    "        ],\n",
    "        \n",
    "        \n",
    "        #weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'text': 0.9,\n",
    "            'stats': 1.5,\n",
    "        },\n",
    "    \n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:04:53.993692Z",
     "start_time": "2019-07-13T17:04:53.681900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 42\n",
    "X = df_news[['text', 'polarity', 'subjectivity', 'len']]\n",
    "y = df_news['category']\n",
    "encoder = LabelEncoder()\n",
    "y= encoder.fit_transform(y)\n",
    "x_train, x_test, y_train,y_test = train_test_split(X, y, test_size=0.2, random_state= seed, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:07:17.288476Z",
     "start_time": "2019-07-13T17:04:58.903975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('union', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('text', Pipeline(memory=None,\n",
       "     steps=[('selector', ItemSelector(key='text')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='conte...'=', sort=True,\n",
       "        sparse=True))]))],\n",
       "       transformer_weights={'text': 0.9, 'stats': 1.5}))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-12T16:54:26.815398Z",
     "start_time": "2019-07-12T16:54:26.813078Z"
    }
   },
   "source": [
    "### Transform and train the Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:09:39.731335Z",
     "start_time": "2019-07-13T17:08:08.029847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the number of features in train and test correspond: (160682, 190148) - (40171, 190148)\n",
      "CPU times: user 1min 31s, sys: 576 ms, total: 1min 32s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "train_vec = pipeline.transform(x_train)\n",
    "test_vec = pipeline.transform(x_test)\n",
    "print(\"Checking that the number of features in train and test correspond: %s - %s\" % (train_vec.shape, test_vec.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:10:00.475882Z",
     "start_time": "2019-07-13T17:10:00.471896Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_sv  = LinearSVC(C=1, class_weight='balanced', multi_class='ovr', random_state=40, max_iter=10000)#Support Vector Machines\n",
    "clf_sgd = SGDClassifier(max_iter=200) #Stochastic Gradient Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:37:35.471916Z",
     "start_time": "2019-07-13T17:23:14.073061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59964908 0.60032112 0.60045568]\n",
      "Mean score: 0.600142 (+/-0.000353)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orah82/anaconda3/envs/DSI/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57607377 0.5796912  0.58009935]\n",
      "Mean score: 0.578621 (+/-0.001809)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clfs = [clf_sv, clf_sgd]\n",
    "cv =3\n",
    "for clf in clfs:\n",
    "    scores = cross_val_score(clf, pipeline.transform(x_train), y_train, cv=cv, scoring =\"accuracy\")\n",
    "    print(scores)\n",
    "    print((\"Mean score: {0:3f} (+/-{1:3f})\").format(\n",
    "    np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:47:26.234726Z",
     "start_time": "2019-07-13T17:37:35.474378Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import  classification_report\n",
    "clf_sv.fit(pipeline.transform(x_train), y_train)\n",
    "y_pred = clf_sv.predict(pipeline.transform(x_test))\n",
    "list_result = []\n",
    "list_result.append(('SVC',accuracy_score(y_test, y_pred)))\n",
    "clf_sgd.fit(pipeline.transform(x_train), y_train)\n",
    "y_pred = clf_sgd.predict(pipeline.transform(x_test))\n",
    "list_result.append((\"SGD\", accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:49:07.514873Z",
     "start_time": "2019-07-13T17:48:21.927236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz#egg=en_core_web_lg==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz (852.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 852.3MB 18.9MB/s ta 0:00:011    46% |███████████████                 | 398.5MB 45.8MB/s eta 0:00:10    58% |██████████████████▉             | 500.5MB 37.9MB/s eta 0:00:10    72% |███████████████████████         | 615.2MB 59.4MB/s eta 0:00:04    76% |████████████████████████▋       | 655.7MB 41.8MB/s eta 0:00:05    82% |██████████████████████████▌     | 705.3MB 37.7MB/s eta 0:00:04    96% |██████████████████████████████▊ | 819.4MB 36.1MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
      "  Running setup.py install for en-core-web-lg ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-lg-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /Users/orah82/anaconda3/envs/dsi/lib/python3.7/site-packages/en_core_web_lg\n",
      "    -->\n",
      "    /Users/orah82/anaconda3/envs/dsi/lib/python3.7/site-packages/spacy/data/en_core_web_lg\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_lg')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net and Spacy Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:52:33.575352Z",
     "start_time": "2019-07-13T17:52:25.524263Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp =spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T22:44:04.871587Z",
     "start_time": "2019-07-14T22:44:04.865010Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM, Embedding\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate, GRU\n",
    "from keras.layers import Reshape, merge, Concatenate, Lambda, Average\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T17:59:54.782325Z",
     "start_time": "2019-07-13T17:59:54.720895Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "X = df_news['text']\n",
    "y = df_news['category']\n",
    "encoder = LabelEncoder()\n",
    "y= encoder.fit_transform(y)\n",
    "Y = np_utils.to_categorical(y)\n",
    "vectorizer = TfidfVectorizer(min_df=3, max_df=0.2, max_features=None,\n",
    "                            strip_accents= 'unicode', analyzer ='word', token_pattern=r'\\w{1,}',\n",
    "                            use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "                            stop_words=None, preprocessor=clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T13:51:14.088676Z",
     "start_time": "2019-07-14T13:50:27.193897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.2, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function clean_text at 0x1a340a12f0>, smooth_idf=1,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=1,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=1,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed =42 \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed,\n",
    "                                                   stratify = y)\n",
    "vectorizer.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T14:02:49.416397Z",
     "start_time": "2019-07-14T14:02:04.384680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9816, 20426, 10559, 25144, 4603, 15407, 20097, 19671, 8892, 19671, 25144, 4603, 26387, 11170, 3994, 12035, 20252, 19001, 9816, 19001]\n"
     ]
    }
   ],
   "source": [
    "word2idx ={word: idx for idx, word in enumerate(vectorizer.get_feature_names())}\n",
    "tokenize = vectorizer.build_tokenizer()\n",
    "preprocess = vectorizer.build_preprocessor()\n",
    "\n",
    "def to_sequence(tokenizer, preprocessor, index, text):\n",
    "    words = tokenizer(preprocessor(text))\n",
    "    indexes = [index[word] for word in words if word in index]\n",
    "    return indexes\n",
    "\n",
    "X_train_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in x_train]\n",
    "print(X_train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T14:14:01.610463Z",
     "start_time": "2019-07-14T14:14:00.945532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30022 30022 30022 30022 30022 30022 30022 30022 30022 30022 30022 30022\n",
      " 30022 30022 30022 30022 30022 30022 30022 30022 30022 30022 30022 30022\n",
      " 30022 30022 30022 30022 30022 30022 30022 30022 30022 30022 30022 30022\n",
      " 30022 30022 30022 30022  9816 20426 10559 25144  4603 15407 20097 19671\n",
      "  8892 19671 25144  4603 26387 11170  3994 12035 20252 19001  9816 19001]\n"
     ]
    }
   ],
   "source": [
    "# Calulate the max length of a text\n",
    "\n",
    "MAX_SEQ_LENGTH = 60\n",
    "\n",
    "N_FEATURES = len(vectorizer.get_feature_names())\n",
    "X_train_sequences = pad_sequences(X_train_sequences, maxlen= MAX_SEQ_LENGTH, value=N_FEATURES)\n",
    "\n",
    "print(X_train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T15:42:30.668381Z",
     "start_time": "2019-07-14T15:42:18.836559Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_sequences = [to_sequence(tokenize, preprocess,word2idx,x) for x in x_test]\n",
    "X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGTH, value=N_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the spacy embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T15:50:37.482218Z",
     "start_time": "2019-07-14T15:50:33.837384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING_LEN= 300\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_LEN = 300\n",
    "\n",
    "embedding_index =np.zeros((len(vectorizer.get_feature_names()) + 1, EMBEDDING_LEN))\n",
    "for word, idx in word2idx.items():\n",
    "    try: \n",
    "        embedding = nlp.vocab[word].vector\n",
    "        embedding_index[idx] = embedding\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(\"EMBEDDING_LEN=\", EMBEDDING_LEN)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:53:58.711932Z",
     "start_time": "2019-07-14T16:53:58.322385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 60, 300)           9006900   \n",
      "_________________________________________________________________\n",
      "unified_lstm_1 (UnifiedLSTM) (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 41)                12341     \n",
      "=================================================================\n",
      "Total params: 9,740,441\n",
      "Trainable params: 733,541\n",
      "Non-trainable params: 9,006,900\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(vectorizer.get_feature_names())+1,\n",
    "                   EMBEDDING_LEN, #Embedding Size\n",
    "                   weights= [embedding_index],\n",
    "                   input_length= MAX_SEQ_LENGTH,\n",
    "                   trainable=False))\n",
    "model.add(LSTM(300, dropout = 0.2))\n",
    "model.add(Dense(len(set(y)), activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T18:49:28.482454Z",
     "start_time": "2019-07-14T17:15:55.321007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144613 samples, validate on 16069 samples\n",
      "Epoch 1/15\n",
      "144613/144613 [==============================] - 340s 2ms/sample - loss: 3.0114 - accuracy: 0.2244 - val_loss: 2.7393 - val_accuracy: 0.2990\n",
      "Epoch 2/15\n",
      "144613/144613 [==============================] - 337s 2ms/sample - loss: 2.6097 - accuracy: 0.3300 - val_loss: 2.4798 - val_accuracy: 0.3697\n",
      "Epoch 3/15\n",
      "144613/144613 [==============================] - 337s 2ms/sample - loss: 2.4164 - accuracy: 0.3771 - val_loss: 2.3259 - val_accuracy: 0.4044\n",
      "Epoch 4/15\n",
      "144613/144613 [==============================] - 343s 2ms/sample - loss: 2.2649 - accuracy: 0.4115 - val_loss: 2.2011 - val_accuracy: 0.4324\n",
      "Epoch 5/15\n",
      "144613/144613 [==============================] - 337s 2ms/sample - loss: 2.1441 - accuracy: 0.4390 - val_loss: 2.1089 - val_accuracy: 0.4539\n",
      "Epoch 6/15\n",
      "144613/144613 [==============================] - 339s 2ms/sample - loss: 2.0484 - accuracy: 0.4615 - val_loss: 2.0257 - val_accuracy: 0.4705\n",
      "Epoch 7/15\n",
      "144613/144613 [==============================] - 336s 2ms/sample - loss: 1.9638 - accuracy: 0.4801 - val_loss: 1.9837 - val_accuracy: 0.4759\n",
      "Epoch 8/15\n",
      "144613/144613 [==============================] - 341s 2ms/sample - loss: 1.8884 - accuracy: 0.4971 - val_loss: 1.9082 - val_accuracy: 0.4969\n",
      "Epoch 9/15\n",
      "144613/144613 [==============================] - 348s 2ms/sample - loss: 1.8185 - accuracy: 0.5131 - val_loss: 1.8735 - val_accuracy: 0.5045\n",
      "Epoch 10/15\n",
      "144613/144613 [==============================] - 348s 2ms/sample - loss: 1.7598 - accuracy: 0.5248 - val_loss: 1.8275 - val_accuracy: 0.5157\n",
      "Epoch 11/15\n",
      "144613/144613 [==============================] - 348s 2ms/sample - loss: 1.7039 - accuracy: 0.5374 - val_loss: 1.7959 - val_accuracy: 0.5214\n",
      "Epoch 12/15\n",
      "144613/144613 [==============================] - 515s 4ms/sample - loss: 1.6554 - accuracy: 0.5483 - val_loss: 1.7830 - val_accuracy: 0.5244\n",
      "Epoch 13/15\n",
      "144613/144613 [==============================] - 338s 2ms/sample - loss: 1.6049 - accuracy: 0.5581 - val_loss: 1.7712 - val_accuracy: 0.5276\n",
      "Epoch 14/15\n",
      "144613/144613 [==============================] - 604s 4ms/sample - loss: 1.5591 - accuracy: 0.5687 - val_loss: 1.7576 - val_accuracy: 0.5315\n",
      "Epoch 15/15\n",
      "144613/144613 [==============================] - 349s 2ms/sample - loss: 1.5182 - accuracy: 0.5781 - val_loss: 1.7334 - val_accuracy: 0.5386\n",
      "40171/40171 [==============================] - 46s 1ms/sample - loss: 1.7282 - accuracy: 0.5392\n",
      "Accuracy: 0.53919494\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_sequences,y_train,\n",
    "          epochs=15, batch_size=150, verbose=1,\n",
    "          validation_split=0.1)\n",
    "\n",
    "scores = model.evaluate(X_test_sequences,y_test, verbose =1)\n",
    "print(\"Accuracy:\", scores[1]) #model score\n",
    "list_result.append((\"LSTM\", scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T19:05:37.120574Z",
     "start_time": "2019-07-14T19:05:37.108920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160682"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model LSTM and concatenate new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T19:44:42.326864Z",
     "start_time": "2019-07-14T19:44:42.324372Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T19:51:25.525399Z",
     "start_time": "2019-07-14T19:51:24.833949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 60, 300)      9006900     text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_2 (UnifiedLSTM)    (None, 300)          721200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "meta (InputLayer)               [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 303)          0           unified_lstm_2[0][0]             \n",
      "                                                                 meta[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 150)          45600       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 150)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_2 (Batch (None, 150)          600         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 41)           6191        batch_normalization_v2_2[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 9,780,491\n",
      "Trainable params: 773,291\n",
      "Non-trainable params: 9,007,200\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text_data = Input(shape=(MAX_SEQ_LENGTH,), name='text')\n",
    "meta_data = Input(shape=(3,),name ='meta')\n",
    "x=(Embedding(len(vectorizer.get_feature_names())+1,\n",
    "                    EMBEDDING_LEN,\n",
    "                    weights=[embedding_index],\n",
    "                    input_length=MAX_SEQ_LENGTH,\n",
    "                    trainable= False))(text_data)\n",
    "x2= ((LSTM(300, dropout=0.2, recurrent_dropout=0.2)))(x)\n",
    "x4= concatenate([x2, meta_data])\n",
    "x5= Dense(150, activation='relu')(x4)\n",
    "x6= Dropout(0.25)(x5)\n",
    "x7=BatchNormalization()(x6)\n",
    "out=(Dense(len(set(y)), activation='softmax'))(x7)\n",
    "model= Model(inputs=[text_data, meta_data], outputs=out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T19:54:41.790524Z",
     "start_time": "2019-07-14T19:54:40.923693Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cat_train = df_news.iloc[x_train.index][['polarity', 'subjectivity', 'len']]\n",
    "df_cat_test = df_news.iloc[x_test.index][['polarity', 'subjectivity', 'len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T22:28:16.266465Z",
     "start_time": "2019-07-14T19:58:23.747182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144613 samples, validate on 16069 samples\n",
      "Epoch 1/12\n",
      "144613/144613 [==============================] - 539s 4ms/sample - loss: 2.8768 - accuracy: 0.2677 - val_loss: 2.6038 - val_accuracy: 0.3351\n",
      "Epoch 2/12\n",
      "144613/144613 [==============================] - 730s 5ms/sample - loss: 2.5927 - accuracy: 0.3330 - val_loss: 2.4290 - val_accuracy: 0.3645\n",
      "Epoch 3/12\n",
      "144613/144613 [==============================] - 593s 4ms/sample - loss: 2.4591 - accuracy: 0.3653 - val_loss: 2.2836 - val_accuracy: 0.4071\n",
      "Epoch 4/12\n",
      "144613/144613 [==============================] - 545s 4ms/sample - loss: 2.3284 - accuracy: 0.3971 - val_loss: 2.1632 - val_accuracy: 0.4351\n",
      "Epoch 5/12\n",
      "144613/144613 [==============================] - 1211s 8ms/sample - loss: 2.2049 - accuracy: 0.4294 - val_loss: 2.1138 - val_accuracy: 0.4489\n",
      "Epoch 6/12\n",
      "144613/144613 [==============================] - 1849s 13ms/sample - loss: 2.0884 - accuracy: 0.4574 - val_loss: 2.0038 - val_accuracy: 0.4751\n",
      "Epoch 7/12\n",
      "144613/144613 [==============================] - 583s 4ms/sample - loss: 1.9977 - accuracy: 0.4779 - val_loss: 1.8993 - val_accuracy: 0.4993\n",
      "Epoch 8/12\n",
      "144613/144613 [==============================] - 535s 4ms/sample - loss: 1.9213 - accuracy: 0.4956 - val_loss: 1.8351 - val_accuracy: 0.5166\n",
      "Epoch 9/12\n",
      "144613/144613 [==============================] - 550s 4ms/sample - loss: 1.8568 - accuracy: 0.5102 - val_loss: 1.8335 - val_accuracy: 0.5133\n",
      "Epoch 10/12\n",
      "144613/144613 [==============================] - 597s 4ms/sample - loss: 1.7954 - accuracy: 0.5218 - val_loss: 1.7485 - val_accuracy: 0.5327\n",
      "Epoch 11/12\n",
      "144613/144613 [==============================] - 580s 4ms/sample - loss: 1.7500 - accuracy: 0.5313 - val_loss: 1.7399 - val_accuracy: 0.5319\n",
      "Epoch 12/12\n",
      "144613/144613 [==============================] - 583s 4ms/sample - loss: 1.7030 - accuracy: 0.5414 - val_loss: 1.6870 - val_accuracy: 0.5458\n",
      "40171/40171 [==============================] - 91s 2ms/sample - loss: 1.6925 - accuracy: 0.5471\n",
      "Accuracy: 0.5470613\n"
     ]
    }
   ],
   "source": [
    "model.fit([X_train_sequences, df_cat_train], y_train,\n",
    "         epochs=12, batch_size=128, verbose=1,\n",
    "         validation_split=0.1)\n",
    "\n",
    "scores= model.evaluate([X_test_sequences, df_cat_test], y_test, verbose=1)\n",
    "print('Accuracy:', scores[1])\n",
    "list_result.append((\"LSTM with Multi-Input\", scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T22:55:32.832304Z",
     "start_time": "2019-07-14T22:55:32.170363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 60, 300)      9006900     text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "unified_lstm_1 (UnifiedLSTM)    (None, 300)          721200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "meta (InputLayer)               [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 303)          0           unified_lstm_1[0][0]             \n",
      "                                                                 meta[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 150)          45600       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 150)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2 (BatchNo (None, 150)          600         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 41)           6191        batch_normalization_v2[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 9,780,491\n",
      "Trainable params: 773,291\n",
      "Non-trainable params: 9,007,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "    \n",
    "\n",
    "x=(Embedding(len(vectorizer.get_feature_names())+1,\n",
    "                    EMBEDDING_LEN,\n",
    "                    weights=[embedding_index],\n",
    "                    input_length=MAX_SEQ_LENGTH,\n",
    "                    trainable= False))(text_data)\n",
    "x2= ((LSTM(300, dropout=0.2, recurrent_dropout=0.2)))(x)\n",
    "x4= concatenate([x2, meta_data])\n",
    "x5= Dense(150, activation='relu')(x4)\n",
    "x6= Dropout(0.25)(x5)\n",
    "x7=BatchNormalization()(x6)\n",
    "out=(Dense(len(set(y)), activation='softmax'))(x7)\n",
    "\n",
    "AttentionLSTM = Model(inputs=[text_data, meta_data ], outputs=out)\n",
    "AttentionLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "AttentionLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T01:04:05.560509Z",
     "start_time": "2019-07-14T22:56:22.653633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 144613 samples, validate on 16069 samples\n",
      "Epoch 1/13\n",
      "144613/144613 [==============================] - 606s 4ms/sample - loss: 2.8915 - acc: 0.2663 - val_loss: 2.5978 - val_acc: 0.3311\n",
      "Epoch 2/13\n",
      "144613/144613 [==============================] - 603s 4ms/sample - loss: 2.6022 - acc: 0.3348 - val_loss: 2.4098 - val_acc: 0.3843\n",
      "Epoch 3/13\n",
      "144613/144613 [==============================] - 597s 4ms/sample - loss: 2.4634 - acc: 0.3680 - val_loss: 2.3278 - val_acc: 0.4049\n",
      "Epoch 4/13\n",
      "144613/144613 [==============================] - 571s 4ms/sample - loss: 2.3326 - acc: 0.4001 - val_loss: 2.2281 - val_acc: 0.4212\n",
      "Epoch 5/13\n",
      "144613/144613 [==============================] - 565s 4ms/sample - loss: 2.2063 - acc: 0.4301 - val_loss: 2.0695 - val_acc: 0.4634\n",
      "Epoch 6/13\n",
      "144613/144613 [==============================] - 560s 4ms/sample - loss: 2.0996 - acc: 0.4556 - val_loss: 1.9860 - val_acc: 0.4783\n",
      "Epoch 7/13\n",
      "144613/144613 [==============================] - 562s 4ms/sample - loss: 2.0070 - acc: 0.4769 - val_loss: 1.9155 - val_acc: 0.4970\n",
      "Epoch 8/13\n",
      "144613/144613 [==============================] - 564s 4ms/sample - loss: 1.9284 - acc: 0.4923 - val_loss: 1.8509 - val_acc: 0.5078\n",
      "Epoch 9/13\n",
      "144613/144613 [==============================] - 567s 4ms/sample - loss: 1.8635 - acc: 0.5064 - val_loss: 1.7912 - val_acc: 0.5269\n",
      "Epoch 10/13\n",
      "144613/144613 [==============================] - 572s 4ms/sample - loss: 1.8059 - acc: 0.5198 - val_loss: 1.7466 - val_acc: 0.5332\n",
      "Epoch 11/13\n",
      "144613/144613 [==============================] - 656s 5ms/sample - loss: 1.7511 - acc: 0.5315 - val_loss: 1.7127 - val_acc: 0.5412\n",
      "Epoch 12/13\n",
      "144613/144613 [==============================] - 562s 4ms/sample - loss: 1.7082 - acc: 0.5407 - val_loss: 1.7503 - val_acc: 0.5336\n",
      "Epoch 13/13\n",
      "144613/144613 [==============================] - 567s 4ms/sample - loss: 1.6660 - acc: 0.5498 - val_loss: 1.7109 - val_acc: 0.5412\n",
      "40171/40171 [==============================] - 99s 2ms/sample - loss: 1.7174 - acc: 0.5384\n",
      "Accuracy: 0.53844815\n"
     ]
    }
   ],
   "source": [
    "AttentionLSTM.fit([X_train_sequences, df_cat_train], y_train, \n",
    "          epochs=13, batch_size=128, verbose=1, \n",
    "          validation_split=0.1)\n",
    " \n",
    "scores = AttentionLSTM.evaluate([X_test_sequences, df_cat_test],y_test, verbose=1)\n",
    "print(\"Accuracy:\", scores[1])  # \n",
    "list_result.append((\"LSTM with Attention\", scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T01:07:07.014861Z",
     "start_time": "2019-07-15T01:07:06.977269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.604939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.581638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.539195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM with Multi-Input</td>\n",
       "      <td>0.547061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LSTM with Attention</td>\n",
       "      <td>0.538448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model  accuracy\n",
       "0                    SVC  0.604939\n",
       "1                    SGD  0.581638\n",
       "2                   LSTM  0.539195\n",
       "3  LSTM with Multi-Input  0.547061\n",
       "4    LSTM with Attention  0.538448"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list_result, columns=['model', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do news articles from different categories have different writing styles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Words by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T01:33:19.083197Z",
     "start_time": "2019-07-15T01:33:03.172611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.2, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function clean_text at 0x1a340a12f0>, smooth_idf=1,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=1,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=1,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer( min_df =3, max_df=0.2, max_features=None, \n",
    "                    strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "                    ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "                    stop_words = None, preprocessor=clean_text)\n",
    "vectorizer.fit(df_news.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T01:34:28.912137Z",
     "start_time": "2019-07-15T01:33:26.508239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 terms for:  CRIME\n",
      "     Terms       CRIME\n",
      "0     home  122.780746\n",
      "1    crime   61.279350\n",
      "2    black   54.603386\n",
      "3    drink   50.149849\n",
      "4     live   47.139003\n",
      "5   parent   42.693406\n",
      "6    women   42.479414\n",
      "7  college   26.158697\n",
      "8     good   24.768079\n",
      "9    money   22.500460\n",
      "###############\n",
      "(10, 2)\n",
      "Top 10 terms for:  ENTERTAINMENT\n",
      "    Terms  ENTERTAINMENT\n",
      "0    live     356.901616\n",
      "1    good     336.142522\n",
      "2   world     331.985059\n",
      "3   women     239.054035\n",
      "4   black     204.679678\n",
      "5    home     153.132509\n",
      "6  comedy     145.462310\n",
      "7   media     114.164769\n",
      "8   voice     109.243520\n",
      "9  parent      79.957844\n",
      "###############\n",
      "(14, 3)\n",
      "Top 10 terms for:  WORLD NEWS\n",
      "      Terms  WORLD NEWS\n",
      "0     world  107.477013\n",
      "1     women   57.656851\n",
      "2     media   30.497552\n",
      "3      live   27.490777\n",
      "4      home   26.794834\n",
      "5      good   15.763163\n",
      "6     black   13.506478\n",
      "7    travel   12.679764\n",
      "8      food   12.658104\n",
      "9  politics   10.013183\n",
      "###############\n",
      "(17, 4)\n",
      "Top 10 terms for:  IMPACT\n",
      "       Terms      IMPACT\n",
      "0      world  335.615110\n",
      "1      women  219.864322\n",
      "2       live  218.918858\n",
      "3       good  108.361593\n",
      "4       food  105.316383\n",
      "5  education   98.093954\n",
      "6       home   90.961893\n",
      "7     impact   74.500094\n",
      "8      money   61.243602\n",
      "9     parent   48.704086\n",
      "###############\n",
      "(19, 5)\n",
      "Top 10 terms for:  POLITICS\n",
      "      Terms    POLITICS\n",
      "0      good  549.780123\n",
      "1     women  526.279567\n",
      "2     world  522.900592\n",
      "3      live  495.965770\n",
      "4     money  440.261775\n",
      "5     media  384.111208\n",
      "6     black  346.513256\n",
      "7  politics  338.804538\n",
      "8      home  257.453927\n",
      "9    travel  225.424263\n",
      "###############\n",
      "(19, 6)\n",
      "Top 10 terms for:  WEIRD NEWS\n",
      "   Terms  WEIRD NEWS\n",
      "0  weird   57.198358\n",
      "1  world   56.325323\n",
      "2   good   52.815470\n",
      "3   home   44.471867\n",
      "4   live   43.578597\n",
      "5  drink   41.767391\n",
      "6   food   25.330107\n",
      "7  sport   15.251457\n",
      "8  taste   14.461086\n",
      "9  women   12.952734\n",
      "###############\n",
      "(22, 7)\n",
      "Top 10 terms for:  BLACK VOICES\n",
      "     Terms  BLACK VOICES\n",
      "0    black   1118.094034\n",
      "1     live    180.726633\n",
      "2    women    140.384312\n",
      "3    world     74.524100\n",
      "4    style     69.524882\n",
      "5     home     57.619787\n",
      "6    voice     51.567081\n",
      "7    media     47.362353\n",
      "8  college     46.473412\n",
      "9  culture     41.034887\n",
      "###############\n",
      "(24, 8)\n",
      "Top 10 terms for:  WOMEN\n",
      "     Terms       WOMEN\n",
      "0    women  997.989694\n",
      "1    world  112.158132\n",
      "2     live   99.899275\n",
      "3     good   76.975060\n",
      "4  culture   44.813376\n",
      "5    media   35.156230\n",
      "6    black   30.227129\n",
      "7     home   27.952820\n",
      "8    voice   27.127239\n",
      "9  college   24.110524\n",
      "###############\n",
      "(24, 9)\n",
      "Top 10 terms for:  COMEDY\n",
      "    Terms      COMEDY\n",
      "0    live  186.979595\n",
      "1   world  103.465579\n",
      "2  comedy   97.451753\n",
      "3    good   93.683279\n",
      "4   women   54.545127\n",
      "5   drink   51.335562\n",
      "6    home   50.589363\n",
      "7   black   50.321627\n",
      "8   media   36.606436\n",
      "9    food   33.041692\n",
      "###############\n",
      "(24, 10)\n",
      "Top 10 terms for:  QUEER VOICES\n",
      "     Terms  QUEER VOICES\n",
      "0    queer    507.368019\n",
      "1     live    230.661365\n",
      "2    world    210.803151\n",
      "3    women    133.680133\n",
      "4   parent    104.638088\n",
      "5     good     89.501813\n",
      "6    black     86.053343\n",
      "7  culture     78.234736\n",
      "8    media     71.953139\n",
      "9     home     70.852188\n",
      "###############\n",
      "(25, 11)\n",
      "Top 10 terms for:  SPORTS\n",
      "     Terms      SPORTS\n",
      "0    sport  261.695984\n",
      "1    world  254.587908\n",
      "2     live  145.112861\n",
      "3  college  100.071823\n",
      "4     good   92.590537\n",
      "5    women   63.387473\n",
      "6     home   59.329693\n",
      "7    green   41.851531\n",
      "8    money   32.116693\n",
      "9    black   28.151404\n",
      "###############\n",
      "(26, 12)\n",
      "Top 10 terms for:  BUSINESS\n",
      "      Terms    BUSINESS\n",
      "0  business  511.510049\n",
      "1     women  268.383934\n",
      "2     world  216.918011\n",
      "3     money  145.204618\n",
      "4      good  137.254177\n",
      "5      live  102.430219\n",
      "6      home  100.833187\n",
      "7     media   84.108686\n",
      "8      food   79.895663\n",
      "9     black   71.490128\n",
      "###############\n",
      "(27, 13)\n",
      "Top 10 terms for:  TRAVEL\n",
      "      Terms       TRAVEL\n",
      "0    travel  1478.095803\n",
      "1     world   818.069789\n",
      "2      home   295.520484\n",
      "3      live   226.784728\n",
      "4      good   220.705020\n",
      "5      food   205.402040\n",
      "6   culture   133.754303\n",
      "7     drink   104.436509\n",
      "8     style    83.880527\n",
      "9  business    79.703414\n",
      "###############\n",
      "(27, 14)\n",
      "Top 10 terms for:  MEDIA\n",
      "      Terms       MEDIA\n",
      "0     media  306.969359\n",
      "1     world   64.188409\n",
      "2      live   60.868360\n",
      "3     women   52.305089\n",
      "4  business   33.109712\n",
      "5      good   29.531488\n",
      "6     black   21.219032\n",
      "7  politics   18.219757\n",
      "8   culture   17.893360\n",
      "9      home   16.261056\n",
      "###############\n",
      "(27, 15)\n",
      "Top 10 terms for:  TECH\n",
      "      Terms        TECH\n",
      "0      tech  110.704609\n",
      "1     world   77.334051\n",
      "2     media   59.622249\n",
      "3      live   42.120952\n",
      "4      good   36.816731\n",
      "5  business   32.899162\n",
      "6     black   29.478155\n",
      "7     women   28.384441\n",
      "8      home   27.868720\n",
      "9     money   23.877565\n",
      "###############\n",
      "(28, 16)\n",
      "Top 10 terms for:  RELIGION\n",
      "      Terms    RELIGION\n",
      "0     world  138.484292\n",
      "1  religion  109.659990\n",
      "2      live   70.954779\n",
      "3      good   62.125046\n",
      "4     women   56.229298\n",
      "5     black   31.723237\n",
      "6      home   24.228565\n",
      "7     voice   18.812460\n",
      "8   science   15.107684\n",
      "9   college   14.929567\n",
      "###############\n",
      "(30, 17)\n",
      "Top 10 terms for:  SCIENCE\n",
      "     Terms     SCIENCE\n",
      "0  science  156.866618\n",
      "1    world   80.531477\n",
      "2     live   58.707733\n",
      "3    black   36.186650\n",
      "4     good   32.697450\n",
      "5    women   28.892333\n",
      "6     home   19.219829\n",
      "7     food   18.939356\n",
      "8   travel   16.295536\n",
      "9   beauty   10.590880\n",
      "###############\n",
      "(31, 18)\n",
      "Top 10 terms for:  LATINO VOICES\n",
      "     Terms  LATINO VOICES\n",
      "0   latino     150.480761\n",
      "1     live      38.716331\n",
      "2    women      26.752656\n",
      "3    world      25.539588\n",
      "4    black      19.091176\n",
      "5     good      14.475711\n",
      "6     home      14.130862\n",
      "7  culture      14.065772\n",
      "8   parent      12.068709\n",
      "9    media      11.262557\n",
      "###############\n",
      "(32, 19)\n",
      "Top 10 terms for:  EDUCATION\n",
      "       Terms   EDUCATION\n",
      "0  education  256.823852\n",
      "1    college   69.517208\n",
      "2     parent   42.450170\n",
      "3      world   39.649946\n",
      "4       good   24.512355\n",
      "5       live   15.577701\n",
      "6      money   15.153713\n",
      "7    science   14.440966\n",
      "8       arts   10.696364\n",
      "9      media   10.459323\n",
      "###############\n",
      "(33, 20)\n",
      "Top 10 terms for:  COLLEGE\n",
      "       Terms     COLLEGE\n",
      "0    college  300.970992\n",
      "1  education   71.909466\n",
      "2       live   29.274479\n",
      "3      world   28.433706\n",
      "4       good   23.809684\n",
      "5      media   21.087161\n",
      "6      women   19.137830\n",
      "7      black   14.505568\n",
      "8      money   14.459631\n",
      "9     parent   13.276724\n",
      "###############\n",
      "(33, 21)\n",
      "Top 10 terms for:  PARENTS\n",
      "     Terms     PARENTS\n",
      "0   parent  707.359366\n",
      "1    world  119.468643\n",
      "2     good  119.457794\n",
      "3     live   93.070973\n",
      "4    women   87.259132\n",
      "5     home   84.427519\n",
      "6  college   32.810194\n",
      "7     food   29.498621\n",
      "8    black   26.205871\n",
      "9    media   25.169830\n",
      "###############\n",
      "(33, 22)\n",
      "Top 10 terms for:  ARTS & CULTURE\n",
      "     Terms  ARTS & CULTURE\n",
      "0    women       83.944272\n",
      "1    world       58.242285\n",
      "2     live       36.184519\n",
      "3    black       24.576167\n",
      "4  culture       19.669249\n",
      "5     good       17.889634\n",
      "6     arts       16.677165\n",
      "7     home       15.342903\n",
      "8   beauty       12.640659\n",
      "9   travel       11.581452\n",
      "###############\n",
      "(33, 23)\n",
      "Top 10 terms for:  STYLE\n",
      "      Terms       STYLE\n",
      "0     style  127.119694\n",
      "1    beauty  121.457273\n",
      "2      good   52.585859\n",
      "3     black   37.889150\n",
      "4     women   34.235477\n",
      "5     world   32.991887\n",
      "6      live   27.325955\n",
      "7     money   16.209290\n",
      "8      home   15.229864\n",
      "9  business   12.270092\n",
      "###############\n",
      "(33, 24)\n",
      "Top 10 terms for:  GREEN\n",
      "         Terms       GREEN\n",
      "0        world  126.067410\n",
      "1         live   81.563213\n",
      "2         home   72.695678\n",
      "3         good   60.881829\n",
      "4        green   54.451319\n",
      "5         food   51.317036\n",
      "6  environment   48.846933\n",
      "7      science   31.090562\n",
      "8       impact   29.409353\n",
      "9     business   23.028661\n",
      "###############\n",
      "(34, 25)\n",
      "Top 10 terms for:  TASTE\n",
      "     Terms       TASTE\n",
      "0     food  195.404255\n",
      "1     good   85.566730\n",
      "2    drink   71.623207\n",
      "3    taste   65.303564\n",
      "4    world   50.008766\n",
      "5  healthy   43.245209\n",
      "6     home   38.952217\n",
      "7    style   19.847067\n",
      "8    green   15.510594\n",
      "9    money   15.262439\n",
      "###############\n",
      "(35, 26)\n",
      "Top 10 terms for:  HEALTHY LIVING\n",
      "     Terms  HEALTHY LIVING\n",
      "0     live      345.903117\n",
      "1     good      265.766372\n",
      "2  healthy      174.907593\n",
      "3    world      151.624279\n",
      "4     food      148.194434\n",
      "5    women      145.051870\n",
      "6     home       78.838935\n",
      "7  science       76.013331\n",
      "8    drink       66.240312\n",
      "9   parent       65.866743\n",
      "###############\n",
      "(35, 27)\n",
      "Top 10 terms for:  THE WORLDPOST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Terms  THE WORLDPOST\n",
      "0   world     173.278804\n",
      "1   women      80.096571\n",
      "2    live      73.967054\n",
      "3   media      64.619845\n",
      "4    home      61.625699\n",
      "5  travel      20.917176\n",
      "6    good      19.760364\n",
      "7   crime      19.265028\n",
      "8    food      18.494040\n",
      "9   voice      14.081013\n",
      "###############\n",
      "(35, 28)\n",
      "Top 10 terms for:  GOOD NEWS\n",
      "     Terms  GOOD NEWS\n",
      "0     home  66.795463\n",
      "1    world  52.287612\n",
      "2     live  47.401297\n",
      "3     good  36.989055\n",
      "4     food  21.952309\n",
      "5    money  18.645634\n",
      "6  college  13.307826\n",
      "7   parent   8.693309\n",
      "8    style   6.000000\n",
      "9    media   5.820593\n",
      "###############\n",
      "(35, 29)\n",
      "Top 10 terms for:  WORLDPOST\n",
      "      Terms   WORLDPOST\n",
      "0     world  212.983439\n",
      "1      live   55.195174\n",
      "2     women   48.587433\n",
      "3      home   28.514555\n",
      "4     media   25.608106\n",
      "5      good   21.343159\n",
      "6  politics   18.544320\n",
      "7  business   17.713685\n",
      "8   culture   17.280580\n",
      "9     voice   14.729446\n",
      "###############\n",
      "(35, 30)\n",
      "Top 10 terms for:  FIFTY\n",
      "     Terms       FIFTY\n",
      "0     live  106.491242\n",
      "1    women   63.274707\n",
      "2     good   62.535528\n",
      "3    world   55.357256\n",
      "4     home   52.630024\n",
      "5   parent   39.731793\n",
      "6  college   20.532550\n",
      "7  healthy   18.100826\n",
      "8   beauty   12.996088\n",
      "9   travel   12.023648\n",
      "###############\n",
      "(35, 31)\n",
      "Top 10 terms for:  ARTS\n",
      "     Terms       ARTS\n",
      "0    world  97.980348\n",
      "1     live  58.421848\n",
      "2     good  30.569801\n",
      "3     arts  30.068446\n",
      "4    women  25.328947\n",
      "5  culture  18.697025\n",
      "6     home  17.715329\n",
      "7   beauty  14.132909\n",
      "8    media  12.387046\n",
      "9    black  11.470254\n",
      "###############\n",
      "(35, 32)\n",
      "Top 10 terms for:  WELLNESS\n",
      "     Terms     WELLNESS\n",
      "0     live  1231.895126\n",
      "1     good   844.720467\n",
      "2  healthy   744.144254\n",
      "3    world   596.013537\n",
      "4     food   570.384152\n",
      "5    women   378.797160\n",
      "6     home   264.296047\n",
      "7    drink   237.881488\n",
      "8  science   150.822923\n",
      "9    money   130.933391\n",
      "###############\n",
      "(35, 33)\n",
      "Top 10 terms for:  PARENTING\n",
      "     Terms    PARENTING\n",
      "0   parent  1657.280034\n",
      "1    world   318.832694\n",
      "2     live   315.552933\n",
      "3     home   294.421745\n",
      "4     good   290.660546\n",
      "5    women   191.797676\n",
      "6  healthy   110.514298\n",
      "7     food   105.605591\n",
      "8  college    94.680319\n",
      "9    media    93.111164\n",
      "###############\n",
      "(35, 34)\n",
      "Top 10 terms for:  HOME & LIVING\n",
      "   Terms  HOME & LIVING\n",
      "0   home    1340.175065\n",
      "1   live     234.053423\n",
      "2  style     110.406988\n",
      "3   good      89.167657\n",
      "4  money      47.671309\n",
      "5  world      39.943329\n",
      "6  green      33.112137\n",
      "7  weird      28.405702\n",
      "8  black      26.395654\n",
      "9   food      22.092890\n",
      "###############\n",
      "(35, 35)\n",
      "Top 10 terms for:  STYLE & BEAUTY\n",
      "    Terms  STYLE & BEAUTY\n",
      "0   style     1794.868477\n",
      "1  beauty      616.221041\n",
      "2   women      244.382390\n",
      "3   black      224.962782\n",
      "4    good      224.492010\n",
      "5   world      191.848867\n",
      "6    live      127.861956\n",
      "7   sport       99.440958\n",
      "8    home       88.385667\n",
      "9   green       65.770821\n",
      "###############\n",
      "(35, 36)\n",
      "Top 10 terms for:  DIVORCE\n",
      "      Terms      DIVORCE\n",
      "0   divorce  1763.607451\n",
      "1    parent   211.686968\n",
      "2     women   104.593126\n",
      "3      good   102.419526\n",
      "4      live    99.235986\n",
      "5      home    53.650073\n",
      "6     world    47.031503\n",
      "7     money    32.371137\n",
      "8     media    25.731036\n",
      "9  business    18.337385\n",
      "###############\n",
      "(36, 37)\n",
      "Top 10 terms for:  WEDDINGS\n",
      "      Terms    WEDDINGS\n",
      "0  weddings  557.264676\n",
      "1      live   96.019948\n",
      "2      good   92.720102\n",
      "3     style   86.688382\n",
      "4     women   80.060442\n",
      "5     world   61.262466\n",
      "6     money   50.592463\n",
      "7      home   48.727319\n",
      "8   divorce   31.890743\n",
      "9    parent   28.700505\n",
      "###############\n",
      "(37, 38)\n",
      "Top 10 terms for:  FOOD & DRINK\n",
      "     Terms  FOOD & DRINK\n",
      "0     food    705.791426\n",
      "1    taste    314.885766\n",
      "2    drink    259.317222\n",
      "3     good    254.239181\n",
      "4    world    137.131272\n",
      "5     home    127.470932\n",
      "6  healthy    109.021498\n",
      "7    green     95.155885\n",
      "8     live     57.666905\n",
      "9    style     44.830466\n",
      "###############\n",
      "(37, 39)\n",
      "Top 10 terms for:  MONEY\n",
      "      Terms       MONEY\n",
      "0     money  239.336428\n",
      "1      home   68.472131\n",
      "2      good   64.824145\n",
      "3      live   42.822717\n",
      "4   college   42.453385\n",
      "5     world   30.033692\n",
      "6     black   29.413934\n",
      "7    parent   28.747800\n",
      "8  business   22.954881\n",
      "9      food   18.404718\n",
      "###############\n",
      "(37, 40)\n",
      "Top 10 terms for:  ENVIRONMENT\n",
      "         Terms  ENVIRONMENT\n",
      "0        world    87.867622\n",
      "1         home    48.743788\n",
      "2         live    46.228985\n",
      "3        green    42.960657\n",
      "4         good    25.616400\n",
      "5         food    21.512942\n",
      "6       impact    19.590190\n",
      "7  environment    19.042585\n",
      "8      science    14.958090\n",
      "9        black    11.213458\n",
      "###############\n",
      "(37, 41)\n",
      "Top 10 terms for:  CULTURE & ARTS\n",
      "     Terms  CULTURE & ARTS\n",
      "0    world       72.493833\n",
      "1     arts       56.101673\n",
      "2  culture       33.698830\n",
      "3     live       29.292515\n",
      "4     good       18.849146\n",
      "5   beauty       17.874212\n",
      "6    black       16.749647\n",
      "7    women       14.989691\n",
      "8    style       13.000000\n",
      "9     home       11.566369\n",
      "###############\n",
      "(37, 42)\n"
     ]
    }
   ],
   "source": [
    "def create_tf_matrix(category):\n",
    "    return vectorizer.transform(df_news[df_news.category == category].text)\n",
    "\n",
    "def create_term_freq(matrix, cat):\n",
    "  category_words = matrix.sum(axis=0)\n",
    "  category_words_freq = [(word, category_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "  return pd.DataFrame(list(sorted(category_words_freq, key = lambda x: x[1], reverse=True)),columns=['Terms', cat])\n",
    "\n",
    "for cat in df_news.category.unique():\n",
    "  print(\"Top 10 terms for: \", cat)\n",
    "  df_right = create_term_freq(create_tf_matrix(cat), cat).head(10)\n",
    "  print(df_right)\n",
    "  print(\"###############\")\n",
    "  if cat != 'CRIME':\n",
    "    df_top5_words = df_top5_words.merge(df_right, how='outer')\n",
    "  else:\n",
    "    df_top5_words = df_right.copy()\n",
    "  print(df_top5_words.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
